{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b7fd9a-4aeb-4534-9073-42eb2b61a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# Utility functions for different split criteria\n",
    "def gini_impurity(y):\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / len(y)\n",
    "    return 1 - np.sum(ps ** 2)\n",
    "\n",
    "def entropy(y):\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / len(y)\n",
    "    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "def misclassification_error(y):\n",
    "    hist = np.bincount(y)\n",
    "    return 1 - np.max(hist) / len(y)\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTreeClassifierFromScratch:\n",
    "    def __init__(self, criterion=\"gini\", max_depth=100, min_samples_split=2, min_impurity_decrease=0.0):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.root = None\n",
    "    \n",
    "    def _criterion_func(self, y):\n",
    "        if self.criterion == \"gini\":\n",
    "            return gini_impurity(y)\n",
    "        elif self.criterion == \"entropy\":\n",
    "            return entropy(y)\n",
    "        elif self.criterion == \"misclassification\":\n",
    "            return misclassification_error(y)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid criterion\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._grow_tree(X, y)\n",
    "    \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "    \n",
    "        # Early stopping criteria\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or n_labels == 1 or n_samples < self.min_samples_split:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return TreeNode(value=leaf_value)\n",
    "        \n",
    "        feat_idx, threshold = self._best_split(X, y, n_features)\n",
    "        if feat_idx is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return TreeNode(value=leaf_value)\n",
    "        \n",
    "        # Calculate the potential gain from this split\n",
    "        parent_impurity = self._criterion_func(y)\n",
    "        left_idxs, right_idxs = self._split(X[:, feat_idx], threshold)\n",
    "        n_left, n_right = len(left_idxs), len(right_idxs)\n",
    "        if n_left == 0 or n_right == 0:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return TreeNode(value=leaf_value)\n",
    "\n",
    "        left_impurity = self._criterion_func(y[left_idxs])\n",
    "        right_impurity = self._criterion_func(y[right_idxs])\n",
    "        child_impurity = (n_left / n_samples) * left_impurity + (n_right / n_samples) * right_impurity\n",
    "\n",
    "        # Stop growing if the impurity decrease is below the threshold\n",
    "        if parent_impurity - child_impurity < self.min_impurity_decrease:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return TreeNode(value=leaf_value)\n",
    "        \n",
    "        # Continue growing the tree\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return TreeNode(feat_idx, threshold, left, right)\n",
    "\n",
    "    def _best_split(self, X, y, n_features):\n",
    "        best_gain = -1\n",
    "        split_idx, split_thresh = None, None\n",
    "        \n",
    "        for feat_idx in range(n_features):\n",
    "            X_column = X[:, feat_idx]\n",
    "            \n",
    "            # Check if the column is numeric or categorical\n",
    "            if np.issubdtype(X_column.dtype, np.number):\n",
    "                thresholds = np.unique(X_column[~np.isnan(X_column)])  # Handle numerical columns\n",
    "            else:\n",
    "                thresholds = np.unique(X_column.astype(str))  # Handle categorical columns\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(y, X_column, threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_thresh = threshold\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _information_gain(self, y, X_column, split_thresh):\n",
    "        parent_loss = self._criterion_func(y)\n",
    "        \n",
    "        left_idxs, right_idxs = self._split(X_column, split_thresh)\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "        \n",
    "        n = len(y)\n",
    "        n_left, n_right = len(left_idxs), len(right_idxs)\n",
    "        e_left, e_right = self._criterion_func(y[left_idxs]), self._criterion_func(y[right_idxs])\n",
    "        child_loss = (n_left / n) * e_left + (n_right / n) * e_right\n",
    "        \n",
    "        return parent_loss - child_loss\n",
    "    \n",
    "    def _split(self, X_column, split_thresh):\n",
    "        if isinstance(split_thresh, str):\n",
    "            left_idxs = np.argwhere(X_column == split_thresh).flatten()\n",
    "            right_idxs = np.argwhere(X_column != split_thresh).flatten()\n",
    "        else:\n",
    "            left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "            right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "    \n",
    "    def _most_common_label(self, y):\n",
    "        return np.bincount(y).argmax()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "        if isinstance(node.threshold, str):\n",
    "            if x[node.feature] == node.threshold:\n",
    "                return self._traverse_tree(x, node.left)\n",
    "            return self._traverse_tree(x, node.right)\n",
    "        else:\n",
    "            if x[node.feature] <= node.threshold:\n",
    "                return self._traverse_tree(x, node.left)\n",
    "            return self._traverse_tree(x, node.right)\n",
    "\n",
    "# Wrapper class for compatibility with Scikit-learn functions\n",
    "class MyDecisionTreeWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, criterion=\"gini\", max_depth=10, min_samples_split=4, min_impurity_decrease=0.0):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.tree = DecisionTreeClassifierFromScratch(criterion=self.criterion, max_depth=self.max_depth, min_samples_split=self.min_samples_split, min_impurity_decrease=self.min_impurity_decrease)\n",
    "        self.classes_ = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.tree.fit(X, y)\n",
    "        self.classes_ = np.unique(y)  # Set the classes_ attribute\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.tree.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1daf2001-a2be-4f4c-93c5-c67a1970383f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'min_samples_split': 2, 'min_impurity_decrease': 0.0, 'max_depth': 5, 'criterion': 'gini'}\n",
      "Best cross-validation accuracy:  0.9256560092900123\n",
      "Final model accuracy on test data after pruning and tuning: 0.7802521696413951\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "# Load and preprocess the data\n",
    "path = 'C:/Users/andre/Downloads/mushroom/secondary_data.csv'\n",
    "df = pd.read_csv(path, delimiter=';')\n",
    "df['class'] = df['class'].map({'p': 1, 'e': 0})\n",
    "\n",
    "# Convert ranges to mean for numerical columns\n",
    "numerical_columns = ['cap-diameter', 'stem-height', 'stem-width']\n",
    "\n",
    "for column in numerical_columns:\n",
    "    df[column] = df[column].apply(lambda x: float(x) if pd.notnull(x) else x)\n",
    "\n",
    "# Impute missing values for numerical columns\n",
    "imputer_numerical = SimpleImputer(strategy='mean')\n",
    "df[numerical_columns] = imputer_numerical.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Define categorical columns\n",
    "categorical_columns = ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', \n",
    "                       'gill-attachment', 'gill-spacing', 'gill-color', 'stem-root', \n",
    "                       'stem-surface', 'stem-color', 'veil-type', 'veil-color', \n",
    "                       'has-ring', 'ring-type', 'spore-print-color', 'habitat', 'season']\n",
    "\n",
    "# Impute missing values for categorical columns with the most frequent value\n",
    "imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "df[categorical_columns] = imputer_categorical.fit_transform(df[categorical_columns])\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = df.drop(columns=['class']).values\n",
    "y = df['class'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# (Optional) Use a subset of the data for faster tuning\n",
    "X_sample, _, y_sample, _ = train_test_split(X_train, y_train, train_size=0.5, random_state=42)\n",
    "\n",
    "# Define a reduced hyperparameter space\n",
    "param_dist = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [5, 10],\n",
    "    'min_samples_split': [2, 4],\n",
    "    'min_impurity_decrease': [0.0, 0.01]\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for faster hyperparameter tuning\n",
    "randomized_search = RandomizedSearchCV(MyDecisionTreeWrapper(), param_dist, n_iter=10, cv=3, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "randomized_search.fit(X_sample, y_sample)  # Use the sample for quicker tuning\n",
    "\n",
    "best_params = randomized_search.best_params_\n",
    "best_score = randomized_search.best_score_\n",
    "\n",
    "print(\"Best parameters found: \", best_params)\n",
    "print(\"Best cross-validation accuracy: \", best_score)\n",
    "\n",
    "# Train the final model using the best-found parameters on the full dataset\n",
    "final_model = MyDecisionTreeWrapper(\n",
    "    criterion=best_params['criterion'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_impurity_decrease=best_params['min_impurity_decrease']\n",
    ")\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "final_predictions = final_model.predict(X_test)\n",
    "\n",
    "# Evaluate the final model accuracy on the test data\n",
    "final_accuracy = np.mean(final_predictions == y_test)\n",
    "print(f\"Final model accuracy on test data after tuning: {final_accuracy_pruned}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199e5cd1-2154-4639-837b-a6eab7d55076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
